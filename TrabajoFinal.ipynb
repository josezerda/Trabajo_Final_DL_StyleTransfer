{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr5qjk7JgNAoJPpxlIDbtU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josezerda/Trabajo_Final_DL_StyleTransfer/blob/main/TrabajoFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargamos las imagenes"
      ],
      "metadata": {
        "id": "25tnpULVIuD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWDLdGgVGLdD",
        "outputId": "9c8ef692-ca69-47c0-8b53-5cf218530256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-26 13:14:57--  https://upload.wikimedia.org/wikipedia/commons/5/52/La_noche_estrellada1.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 208.80.154.240, 2620:0:861:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|208.80.154.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 223725 (218K) [image/jpeg]\n",
            "Saving to: ‘La_noche_estrellada1.jpg’\n",
            "\n",
            "La_noche_estrellada 100%[===================>] 218.48K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-02-26 13:14:57 (2.26 MB/s) - ‘La_noche_estrellada1.jpg’ saved [223725/223725]\n",
            "\n",
            "--2024-02-26 13:14:58--  https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Neckarfront_T%C3%BCbingen_Mai_2017.jpg/775px-Neckarfront_T%C3%BCbingen_Mai_2017.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 208.80.154.240, 2620:0:861:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|208.80.154.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153015 (149K) [image/jpeg]\n",
            "Saving to: ‘775px-Neckarfront_Tübingen_Mai_2017.jpg’\n",
            "\n",
            "775px-Neckarfront_T 100%[===================>] 149.43K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-02-26 13:14:58 (1.79 MB/s) - ‘775px-Neckarfront_Tübingen_Mai_2017.jpg’ saved [153015/153015]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Imagen para estilo\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/5/52/La_noche_estrellada1.jpg\n",
        "\n",
        "# Imagen para contenido\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Neckarfront_T%C3%BCbingen_Mai_2017.jpg/775px-Neckarfront_T%C3%BCbingen_Mai_2017.jpg\n",
        "\n",
        "# Creamos el directorio para los archivos de salida\n",
        "!mkdir /content/output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importamos librerias"
      ],
      "metadata": {
        "id": "sQt6LdrdLdL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "import numpy as np\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import time\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.applications import vgg19\n",
        "from keras import backend as K\n",
        "from pathlib import Path\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "24acassIMAeC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definimos las imagenes que vamos a utilizar, y el directorio de salida"
      ],
      "metadata": {
        "id": "w5BFImQqOZXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_image_path = Path(\"/content/775px-Neckarfront_Tübingen_Mai_2017.jpg\")\n",
        "style_reference_image_path = Path(\"/content/La_noche_estrellada1.jpg\")\n",
        "result_prefix = Path(\"/content/output\")\n",
        "iterations = 100"
      ],
      "metadata": {
        "id": "6EzSTmcPOhPI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) En base a lo visto en el paper ¿Qué significan los parámetros definidos en la siguiente celda?"
      ],
      "metadata": {
        "id": "N3vgOvtR02JI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_variation_weight = 0.1\n",
        "style_weight = 10\n",
        "content_weight = 1"
      ],
      "metadata": {
        "id": "anukFxpz1jCI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el tamaño de las imágenes a utilizar\n",
        "width, height = load_img(base_image_path).size\n",
        "img_nrows = 400\n",
        "img_ncols = int(width * img_nrows / height)"
      ],
      "metadata": {
        "id": "Hmw7Ajufkokc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "Teniendo en cuenta que Style Transfer consiste en generar una imagen a partir de 2 imagenes (de 1 saca el estilo y de la otra el contenido). Esto se logra mediante la optimización de la loss function que tiene 3 componentes:\n",
        "- total_variation_weight = Este valor impone una continuidad espacial local entre los píxeles de la imagen combinada lo que le da coherencia visual\n",
        "\n",
        "- style_weight = Aca es donde aplicamos Deep Learning mediante el uso de una red CNN. Precisamente, consiste en una suma de distancias L2 entre las matrices de Gram de las representaciones de la imagen base y la imagen de referencia de estilo, extraídas de diferentes capas de una CNN (entrenado en ImageNet). La idea general es capturar información de color/textura a diferentes escalas espaciales (escalas bastante grandes –definidas por la profundidad de la capa considerada).\n",
        "\n",
        "- content_weight = Este peso es la distancia L2 entre las características de la imagen base (extraída de una capa profunda) y las características de la imagen combinada, manteniendo la imagen generada lo suficientemente cerca de la original."
      ],
      "metadata": {
        "id": "CLkeU1fw0_qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Explicar qué hace la siguiente celda. En especial las últimas dos líneas de la función antes del return. ¿Por qué?"
      ],
      "metadata": {
        "id": "7rWHKpjS84GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img"
      ],
      "metadata": {
        "id": "7X4GEiLD88Oa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "La funcion \"preprocess_image\", que tiene como argumento el path de una imagen, tiene como tarea realizar el preprocesamiento de una imagen a fin de que la misma pueda ser pasada a la CNN de 19 capas VGG19.\n",
        "\n",
        "**img = np.expand_dims(img, axis=0):** A la imagen (img) que esta representada en el formato de un array de Numpy, se le agrega una nueva dimension al comienzo del mismo. Esto es necesario porque VGG19 espera un batch de imágenes como entrada, incluso si solo estás procesando una imagen. La nueva dimensión representa el tamaño del batch, que es 1 en este caso.\n",
        "\n",
        "**img = vgg19.preprocess_input(img):** Aplica pasos de preprocesamiento específicos diseñados para el modelo VGG19. Estos pasos son:\n",
        "- Resta los valores RGB medios del conjunto de datos ImageNet (con lo que se entreno VGG19) a cada píxel.\n",
        "- Divide cada píxel por el valor de desviación estándar.\n",
        "- Normaliza los valores de cada pixel [0,1].\n",
        "- Conversión del espacio de color de RGB a BGR (si el modelo se entrenó en imágenes BGR)."
      ],
      "metadata": {
        "id": "z2mgybkn9Hyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Habiendo comprendido lo que hace la celda anterior, explique de manera muy concisa qué hace la siguiente celda. ¿Qué relación tiene con la celda anterior?"
      ],
      "metadata": {
        "id": "cdeiwj19a3JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deprocess_image(x):\n",
        "    x = x.reshape((img_nrows, img_ncols, 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "metadata": {
        "id": "tM1YZMqfohNy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "La funcion \"deprocess_image\", revierte el procesamiento realizado por la funcion de la celda anterior \"preprocess_image\" para que la misma pueda ser utilizada por el modelo preentrenado VGG19. La funcion \"deprocess_image\" toma una imagen representada como un array Numpy y la transforma para que vuelva a ser interpretada por el ojo humano.\n"
      ],
      "metadata": {
        "id": "F5VpUNpkolvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get tensor representations of our images\n",
        "# K.variable convierte un numpy array en un tensor, para\n",
        "base_image = K.variable(preprocess_image(base_image_path))\n",
        "style_reference_image = K.variable(preprocess_image(style_reference_image_path))"
      ],
      "metadata": {
        "id": "KIoaWHZssbAr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combination_image = K.placeholder((1, img_nrows, img_ncols, 3))"
      ],
      "metadata": {
        "id": "-XVRCxu2sgd9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aclaración:\n",
        "\n",
        "La siguiente celda sirve para procesar las tres imagenes (contenido, estilo y salida) en un solo batch."
      ],
      "metadata": {
        "id": "dofJu91TsnqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine the 3 images into a single Keras tensor\n",
        "input_tensor = K.concatenate([base_image,\n",
        "                              style_reference_image,\n",
        "                              combination_image], axis=0)"
      ],
      "metadata": {
        "id": "q6_68INesqb4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# build the VGG19 network with our 3 images as input\n",
        "# the model will be loaded with pre-trained ImageNet weights\n",
        "model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet', include_top=False)\n",
        "print('Model loaded.')\n",
        "\n",
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ0r1kBmsuS_",
        "outputId": "5119dc1e-421e-4e73-b1ec-00c7a03e8853"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) En la siguientes celdas:\n",
        "- ¿Qué es la matriz de Gram?¿Para qué se usa?\n",
        "- ¿Por qué se permutan las dimensiones de x?"
      ],
      "metadata": {
        "id": "1Haicro4s0_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(x):\n",
        "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = K.dot(features, K.transpose(features))\n",
        "    return gram"
      ],
      "metadata": {
        "id": "v4Ns6GEftQ22"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "La matriz de Gram se utiliza para captura el estilo de una imagen.\n",
        "La matriz de Gram se calcula tomando los productos internos de las características extraídas de una capa de una red CNN (VGG19). Estas características son representaciones de alto nivel de la imagen original en el espacio de características de la red neuronal. Al calcular el producto interno de estas características, se obtiene información sobre las correlaciones entre las diferentes características.\n",
        "En Neural Style Transfer, las matrices de Gram se calculan tanto para la imagen de estilo como para la imagen generada. El objetivo es generar una imagen que conserve el contenido de una imagen de contenido mientras adopta el estilo de una imagen de estilo. Al comparar las matrices de Gram de la imagen de estilo y la imagen generada en múltiples capas de CNN (VGG19), el algoritmo puede ajustar la imagen generada para que coincida con el estilo de la imagen de estilo. Esto generalmente se hace minimizando la diferencia (por ejemplo, usando un error cuadrático medio u otras funciones de loss) entre las matrices de Gram de la imagen de estilo y la imagen generada a traves de las diferentes capas.\n",
        "\n",
        "En Neural Style Transfer, se permutan las dimensiones de x para calcular la matriz de Gram porque nos interesa calcular las correlaciones entre las características de estilo de una imagen, y la permutación facilita este cálculo.\n",
        "\n",
        "Al permutar las dimensiones de x, transformamos el tensor de características en una matriz donde las filas representan los diferentes canales y las columnas representan la concatenación de las características espaciales. Esto nos permite calcular la matriz de Gram tomando el producto interno de estas características, lo que revela cómo se relacionan entre sí los diferentes canales de características.  Al permutar las dimensiones de x, podemos calcular eficazmente estas correlaciones y, por lo tanto, extraer información crucial sobre el estilo de la imagen de referencia. Esto es fundamental para lograr una transferencia de estilo efectiva en Neural Style Transfer.\n"
      ],
      "metadata": {
        "id": "ntD5KefQ_qU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Losses:\n",
        "- Explicar qué mide cada una de las losses en las siguientes tres celdas."
      ],
      "metadata": {
        "id": "BXQ_eShEr_ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def style_loss(style, combination):\n",
        "    assert K.ndim(style) == 3\n",
        "    assert K.ndim(combination) == 3\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_nrows * img_ncols\n",
        "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
      ],
      "metadata": {
        "id": "aWHfUkELsRYy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "La funcion \"style_loss\" cuantifica qué tan bien la imagen de salida coincide con las características de estilo de la imagen de referencia.\n",
        "\n",
        "**Proposito de la funcion \"style_loss\"**\n",
        "Al minimizar la funcion \"style_loss\", fomentamos que la imagen combinada coincida con las características de estilo (texturas, patrones) de la imagen de estilo. Las características de la imagen de contenido (capturadas por la funcion \"content_loss\") no varian.\n",
        "\n",
        "**Argumentos de entrada:**\n",
        "- style: Representa la imagen de estilo (generalmente una obra de arte)\n",
        "- combination: Representa la imagen combinada (imagen contenido + imagen de estilo).\n",
        "\n",
        "**Uso de la matriz Gram:**\n",
        "La matriz de Gram captura las correlaciones entre los diferentes mapas de características (canales) en el tensor. Se utiliza para medir la similitud de estilo entre la imagen de estilo y la imagen combinada.\n",
        "\n",
        "**Calculo de funcion de Style Loss**\n",
        "La funcion de \"style_loss\" se calcula como la diferencia cuadrática media entre las matrices de Gram de la imagen de estilo (S) y la imagen combinada (C).\n",
        "Se tiene en cuenta\n",
        "- Numero de canales (generalmente 3 para  imagenes RGB )\n",
        "- Numero total de pixels in the image (height x width).\n"
      ],
      "metadata": {
        "id": "O2Pu7goxsg0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def content_loss(base, combination):\n",
        "    return K.sum(K.square(combination - base))"
      ],
      "metadata": {
        "id": "BGBG16nRsSho"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "\n",
        "La funcion \"content_loss\" cuantifica qué tan bien la imagen de salida coincide con las características de la imagen de contenido.\n",
        "\n",
        "**Proposito de la funcion \"content_loss\"**\n",
        "\n",
        "La función \"content_loss\" nos garantiza que la imagen generada conserve los detalles estructurales esenciales de la imagen del contenido al tiempo que incorpora el estilo deseado.\n",
        "\n",
        "**Argumentos de entrada:**\n",
        "\n",
        "- base: Representa la imagen original del contenido (feature map).\n",
        "- combination: Representa la imagen generada (feature map).\n",
        "\n",
        "**Calculo de funcion de Style Loss**\n",
        "\n",
        "La funcion \"content_loss\" calcula la suma de las diferencias al cuadrado entre los elementos correspondientes de base y combinación. Cuantifica qué tan bien el contenido generado coincide con el contenido de referencia.\n",
        "En Neural Style Transfer, minimizar la content loss ayuda a garantizar que la imagen generada capture las características esenciales de la imagen de contenido y al mismo tiempo permita variaciones artísticas."
      ],
      "metadata": {
        "id": "5qntYv0BskUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def total_variation_loss(x):\n",
        "    assert K.ndim(x) == 4\n",
        "    a = K.square(\n",
        "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
        "    b = K.square(\n",
        "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
        "    return K.sum(K.pow(a + b, 1.25))"
      ],
      "metadata": {
        "id": "_0fcseWxsVo2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "**Proposito:**\n",
        "La funcion \"total_variation_loss\" es usada en Neural Style Transfer y ayuda a crear imagenes visualmente coherentes imponiendo una continuidad espacial entre los pixeles adyacentes.\n",
        "\n",
        "**Argumentos de entrada:**\n",
        "\n",
        "x: Un tensor 4D que representa la imagen generada.\n",
        "\n",
        "**Calculo de funcion de la Total Loss**\n",
        "\n",
        "Esta funcion calcula la diferencia cuadrada entre pixels adyacentes a lo largo de ambos ejes (horizontal and vertical). Al minimizar la \"total_variation_loss\", procuramos que la imagen generada tenga transiciones más suaves entre los píxeles vecinos.\n"
      ],
      "metadata": {
        "id": "HSvC5OIpsmAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Armamos la loss total\n",
        "loss = K.variable(0.0)\n",
        "layer_features = outputs_dict['block5_conv2']\n",
        "base_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "loss = loss + content_weight * content_loss(base_image_features,\n",
        "                                            combination_features)\n",
        "\n",
        "feature_layers = ['block1_conv1', 'block2_conv1',\n",
        "                  'block3_conv1', 'block4_conv1',\n",
        "                  'block5_conv1']\n",
        "for layer_name in feature_layers:\n",
        "    layer_features = outputs_dict[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    sl = style_loss(style_reference_features, combination_features)\n",
        "    loss = loss + (style_weight / len(feature_layers)) * sl\n",
        "loss = loss + total_variation_weight * total_variation_loss(combination_image)"
      ],
      "metadata": {
        "id": "w1WcP381cHXe"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Explique el propósito de las siguientes tres celdas. ¿Qué hace la función fmin_l_bfgs_b? ¿En qué se diferencia con la implementación del paper? ¿Se puede utilizar alguna alternativa?"
      ],
      "metadata": {
        "id": "3BDWur0zjRbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grads = K.gradients(loss, combination_image)\n",
        "\n",
        "outputs = [loss]\n",
        "if isinstance(grads, (list, tuple)):\n",
        "    outputs += grads\n",
        "else:\n",
        "    outputs.append(grads)\n",
        "\n",
        "f_outputs = K.function([combination_image], outputs)"
      ],
      "metadata": {
        "id": "ppsjVolycL-R"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "\n",
        " Este código prepara una función que puede calcular tanto la función de loss como los gradientes de esa función de loss con respecto a la imagen combinada, lo que será útil para el proceso de optimización en el que se ajusta la imagen de combinación para que coincida con el estilo y el contenido deseados.\n",
        "\n",
        "Cálculo de los gradientes:\n",
        "\n",
        "\"grads = K.gradients(loss, combination_image)\": Esta línea utiliza la función K.gradients de Keras (o TensorFlow) para calcular los gradientes de la función de loss \"loss\" con respecto a la imagen de combinación \"combination_image\". Aquí, loss es la función de loss que se utiliza para medir la diferencia entre la imagen generada y la imagen de estilo, \"combination_image\" es la variable sobre la cual queremos calcular los gradientes, que es la imagen que estamos optimizando para que coincida con el estilo de la imagen de estilo y el contenido de la imagen de contenido.\n",
        "\n",
        "Armado de las salidas:\n",
        "\n",
        "\"outputs = [loss]\": Se crea una lista llamada outputs que contiene inicialmente solo la función de pérdida loss.\n",
        "\"if isinstance(grads, (list, tuple)): outputs += grads\": Se verifica si grads es una lista o tupla (lo que implica que hay múltiples gradientes). Si es así, se agregan todos los gradientes a la lista outputs.\n",
        "\"else: outputs.append(grads)\": Si grads no es una lista o tupla, se asume que es un solo gradiente y se agrega a la lista outputs.\n",
        "\n",
        "Definición de la función de salida:\n",
        "\n",
        "\"f_outputs = K.function([combination_image], outputs)\": Finalmente, se utiliza K.function para crear una función llamada \"f_outputs\" que toma la imagen de combinación como entrada y devuelve las salidas definidas anteriormente (que incluyen la función de pérdida y los gradientes, si existen) cuando se evalúa con la imagen de combinación."
      ],
      "metadata": {
        "id": "sam5dwadsZKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_loss_and_grads(x):\n",
        "    x = x.reshape((1, img_nrows, img_ncols, 3))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    if len(outs[1:]) == 1:\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "    else:\n",
        "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
        "    return loss_value, grad_values\n",
        "\n",
        "# this Evaluator class makes it possible\n",
        "# to compute loss and gradients in one pass\n",
        "# while retrieving them via two separate functions,\n",
        "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
        "# requires separate functions for loss and gradients,\n",
        "# but computing them separately would be inefficient."
      ],
      "metadata": {
        "id": "5EbryQdVkmMa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "\n",
        "Función: \"eval_loss_and_grads\"\n",
        "Esta función se utiliza en el contexto de optimización de Neural Style Transfer.\n",
        "Su objetivo es calcular tanto la funcion de loss del contenido (loss) como los gradientes asociados a una imagen de entrada x.\n",
        "\n",
        "Parámetros:\n",
        "x: Representa una imagen (generalmente generada por un modelo) que se desea evaluar.\n",
        "\n",
        "Funcionalidad\n",
        "\n",
        "x se redimensiona a una forma específica: (1, img_nrows, img_ncols, 3). Esto asegura que la entrada sea un tensor 4D con un solo ejemplo.\n",
        "Se llama a la función \"f_outputs\" con x como argumento. Esta función devuelve una lista de valores (loss y gradientes).\n",
        "Se extrae el valor de loss (loss_value) de la primera posición de la lista outs.\n",
        "Si hay más elementos en la lista outs, se asume que el segundo elemento es el gradiente. En ese caso, se aplana y se convierte a tipo float64.\n",
        "Si hay más de dos elementos en outs, se consideran como otros valores (por ejemplo, más gradientes) y se aplana y convierte en un arreglo de tipo float64.\n",
        "Finalmente, la función devuelve tanto el valor de loss como los gradientes.\n",
        "Uso en Neural Style Transfer:\n",
        "Durante la optimización, se necesita evaluar la pérdida y los gradientes para ajustar la imagen generada. Los gradientes se utilizan para actualizar la imagen y lograr una mejor combinación de estilo y contenido.\n",
        "\n",
        "En resumen, esta función toma una imagen, la preprocesa, llama a la función f_outputs para obtener los valores de loss y los gradientes, extrae los valores individuales y los devuelve ambos."
      ],
      "metadata": {
        "id": "jr0YZxtsrOkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values"
      ],
      "metadata": {
        "id": "maSvhiA1217o"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Respuesta:\n",
        "\n",
        "Clase: Evaluator\n",
        "La clase Evaluator se utiliza en el contexto de optimizaciónen neural style transfer.\n",
        "Su objetivo es calcular tanto la funcion de loss del contenido (loss) como los gradientes asociados a una imagen de entrada.\n",
        "\n",
        "Métodos:\n",
        "__init__(self): El método de inicialización (__init__) crea una instancia de la clase Evaluator. Inicializa dos atributos: loss_value y grads_values, ambos inicialmente como None.\n",
        "\n",
        "loss(self, x): Este método calcula el valor de la funcion de loss (loss) para una imagen de entrada x. Utiliza la función eval_loss_and_grads(x) para obtener tanto la loss como los gradientes. Luego, almacena estos valores en los atributos de la instancia y devuelve el valor de la funcion de loss.\n",
        "\n",
        "grads(self, x): Este método devuelve los gradientes previamente calculados. Antes de hacerlo, verifica que la funcion de loss ya haya sido calculada (es decir, self.loss_value no sea None). Luego, copia los gradientes almacenados y restablece los atributos de la instancia.\n",
        "\n",
        "Uso en Neural Style Transfer:\n",
        "Durante la optimización, se necesita evaluar la funcion de loss y los gradientes para ajustar la imagen generada.\n",
        "La clase Evaluator encapsula esta funcionalidad, permitiendo calcular la funcion de loss y los gradientes de manera eficiente.\n",
        "En resumen, la clase Evaluator es una parte importante del proceso de optimización en algoritmos como neural style transfer. Ayuda a mantener el estado de los valores de la funcion de loss y gradientes mientras se ajusta la imagen generada."
      ],
      "metadata": {
        "id": "v0bGkrNh5tny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Qué hace la función fmin_l_bfgs_b?\n",
        "\n",
        "La \"función fmin_l_bfgs_b\" es parte de la biblioteca SciPy y se utiliza para minimizar una función utilizando el algoritmo L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with box constraints).\n",
        "\n",
        "Objetivo:\n",
        "Minimiza una función dada utilizando el algoritmo L-BFGS-B.\n",
        "Este algoritmo es especialmente útil para problemas de optimización con restricciones de límites en las variables.\n",
        "\n",
        "Parámetros:\n",
        "func: La función que se desea minimizar.\n",
        "x0: La estimación inicial de la solución.\n",
        "fprime: El gradiente de la función (opcional).\n",
        "bounds: Pares (mínimo, máximo) para cada elemento en x, definiendo los límites en esa dirección.\n",
        "Otros parámetros como factr, pgtol, maxfun, maxiter, etc.\n",
        "\n",
        "Salida:\n",
        "x: La posición estimada del mínimo.\n",
        "f: El valor de la función en el mínimo.\n",
        "Información adicional en un diccionario.\n",
        "\n",
        "En resumen, fmin_l_bfgs_b es una herramienta poderosa para resolver problemas de optimización con restricciones. Ayuda a encontrar los valores óptimos de las variables mientras cumple con las limitaciones impuestas."
      ],
      "metadata": {
        "id": "NCCOod16FI2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hyPLtZTPG65C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿En qué se diferencia con la implementación del paper?\n",
        "\n",
        "Diferencias:\n",
        "\n",
        "- El paper de Gatys propone un enfoque completamente diferente para combinar contenido y estilo en imágenes artísticas.\n",
        "- En lugar de utilizar métodos de optimización como L-BFGS-B, el algoritmo de Gatys se basa en redes neuronales convolucionales (CNN) preentrenadas.\n",
        "- El algoritmo de Gatys utiliza representaciones neuronales para separar y recombinar contenido y estilo de imágenes arbitrarias.\n",
        "- A través de estas representaciones, logra crear imágenes artísticas de alta calidad que capturan tanto el contenido como el estilo deseado.\n",
        "\n",
        "En resumen, mientras que fmin_l_bfgs_b es una herramienta general para optimización, el algoritmo de Gatys es específico para la transferencia de estilo artístico y utiliza redes neuronales para lograr resultados"
      ],
      "metadata": {
        "id": "PVGKBd9sHxvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Se puede utilizar alguna alternativa?\n",
        "\n",
        "\n",
        "Sí, existen algunas alternativas a la función \"fmin_l_bfgs_b\" para la optimización en Neural Style Transfer:\n",
        "\n",
        "1. Optimizadores Adam y SGD:\n",
        "El optimizador Adam (Adaptive Moment Estimation) y el SGD (Stochastic Gradient Descent) son dos algoritmos populares para la optimización en redes neuronales.\n",
        "Ambos ofrecen ventajas sobre fmin_l_bfgs_b:\n",
        "Mayor velocidad de convergencia: Adam y SGD pueden converger más rápido que fmin_l_bfgs_b, especialmente en problemas con grandes conjuntos de datos.\n",
        "Facilidad de implementación: Son más fáciles de implementar y configurar que fmin_l_bfgs_b.\n",
        "Sin embargo, pueden tener algunos inconvenientes:\n",
        "Menor precisión: Es posible que no alcancen la misma precisión que fmin_l_bfgs_b en algunos casos.\n",
        "Sensibilidad a la tasa de aprendizaje: La elección de la tasa de aprendizaje puede ser crucial para el éxito de estos algoritmos.\n",
        "\n",
        "2. Optimizadores de segundo orden:\n",
        "Existen optimizadores de segundo orden como L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) que pueden ofrecer mayor precisión que fmin_l_bfgs_b.\n",
        "Sin embargo, son más costosos computacionalmente y pueden no ser adecuados para grandes conjuntos de datos.\n",
        "\n",
        "3. Scipy’s \"minimize\" con otros métodos:\n",
        "La biblioteca Scipy proporciona varios otros métodos de optimización además de L-BFGS-B.\n",
        "Algoritmos como trust-constr, SLSQP, COBYLA.\n",
        "\n",
        "4. Algoritmos Evolutivos:\n",
        "Los algoritmos evolutivos, como el algoritmo de optimización de enjambre de partículas (PSO) o el algoritmo genético, también pueden ser alternativas interesantes.\n",
        "Estos métodos se inspiran en la evolución biológica y pueden ser útiles para problemas de optimización más complejos.\n",
        "\n",
        "5. Bibliotecas de optimización:\n",
        "Existen bibliotecas de optimización como TensorFlow Optimizers y PyTorch Optimizers que ofrecen una amplia gama de algoritmos para la optimización.\n",
        "\n",
        "La elección del mejor método depende de varios factores:\n",
        "- Tamaño del conjunto de datos: Adam y SGD son más eficientes para grandes conjuntos de datos.\n",
        "- Precisión requerida: Si se requiere la máxima precisión, fmin_l_bfgs_b o un optimizador de segundo orden puede ser mejor.\n",
        "- Tiempo disponible: Si el tiempo es limitado, Adam o SGD pueden ser mejores opciones.\n"
      ],
      "metadata": {
        "id": "yI8KSbLqKFEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Ejecute la siguiente celda y observe las imágenes de salida en cada iteración."
      ],
      "metadata": {
        "id": "R_p0azIBh4Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = Evaluator()\n",
        "\n",
        "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss\n",
        "x = preprocess_image(base_image_path)\n",
        "\n",
        "for i in range(iterations):\n",
        "    print('Start of iteration', i)\n",
        "    start_time = time.time()\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
        "                                     fprime=evaluator.grads, maxfun=20)\n",
        "    print('Current loss value:', min_val)\n",
        "    # save current generated image\n",
        "    img = deprocess_image(x.copy())\n",
        "    fname = result_prefix / ('output_at_iteration_%d.png' % i)\n",
        "    save_img(fname, img)\n",
        "    end_time = time.time()\n",
        "    print('Image saved as', fname)\n",
        "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw-SqPLfh76q",
        "outputId": "eaf0500d-44a5-4415-af7a-475a4be219ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of iteration 0\n",
            "Current loss value: 12398301000.0\n",
            "Image saved as /content/output/output_at_iteration_0.png\n",
            "Iteration 0 completed in 504s\n",
            "Start of iteration 1\n",
            "Current loss value: 6042097000.0\n",
            "Image saved as /content/output/output_at_iteration_1.png\n",
            "Iteration 1 completed in 471s\n",
            "Start of iteration 2\n",
            "Current loss value: 4105960200.0\n",
            "Image saved as /content/output/output_at_iteration_2.png\n",
            "Iteration 2 completed in 476s\n",
            "Start of iteration 3\n",
            "Current loss value: 3186917600.0\n",
            "Image saved as /content/output/output_at_iteration_3.png\n",
            "Iteration 3 completed in 464s\n",
            "Start of iteration 4\n",
            "Current loss value: 2601488600.0\n",
            "Image saved as /content/output/output_at_iteration_4.png\n",
            "Iteration 4 completed in 466s\n",
            "Start of iteration 5\n",
            "Current loss value: 2250572300.0\n",
            "Image saved as /content/output/output_at_iteration_5.png\n",
            "Iteration 5 completed in 466s\n",
            "Start of iteration 6\n",
            "Current loss value: 2018009600.0\n",
            "Image saved as /content/output/output_at_iteration_6.png\n",
            "Iteration 6 completed in 462s\n",
            "Start of iteration 7\n",
            "Current loss value: 1853033000.0\n",
            "Image saved as /content/output/output_at_iteration_7.png\n",
            "Iteration 7 completed in 458s\n",
            "Start of iteration 8\n",
            "Current loss value: 1749188100.0\n",
            "Image saved as /content/output/output_at_iteration_8.png\n",
            "Iteration 8 completed in 461s\n",
            "Start of iteration 9\n",
            "Current loss value: 1661111300.0\n",
            "Image saved as /content/output/output_at_iteration_9.png\n",
            "Iteration 9 completed in 461s\n",
            "Start of iteration 10\n",
            "Current loss value: 1584545000.0\n",
            "Image saved as /content/output/output_at_iteration_10.png\n",
            "Iteration 10 completed in 463s\n",
            "Start of iteration 11\n",
            "Current loss value: 1528769700.0\n",
            "Image saved as /content/output/output_at_iteration_11.png\n",
            "Iteration 11 completed in 462s\n",
            "Start of iteration 12\n",
            "Current loss value: 1483479000.0\n",
            "Image saved as /content/output/output_at_iteration_12.png\n",
            "Iteration 12 completed in 461s\n",
            "Start of iteration 13\n",
            "Current loss value: 1445420300.0\n",
            "Image saved as /content/output/output_at_iteration_13.png\n",
            "Iteration 13 completed in 457s\n",
            "Start of iteration 14\n",
            "Current loss value: 1411898100.0\n",
            "Image saved as /content/output/output_at_iteration_14.png\n",
            "Iteration 14 completed in 457s\n",
            "Start of iteration 15\n",
            "Current loss value: 1379006500.0\n",
            "Image saved as /content/output/output_at_iteration_15.png\n",
            "Iteration 15 completed in 459s\n",
            "Start of iteration 16\n",
            "Current loss value: 1344154800.0\n",
            "Image saved as /content/output/output_at_iteration_16.png\n",
            "Iteration 16 completed in 460s\n",
            "Start of iteration 17\n",
            "Current loss value: 1317728900.0\n",
            "Image saved as /content/output/output_at_iteration_17.png\n",
            "Iteration 17 completed in 473s\n",
            "Start of iteration 18\n",
            "Current loss value: 1279684200.0\n",
            "Image saved as /content/output/output_at_iteration_18.png\n",
            "Iteration 18 completed in 508s\n",
            "Start of iteration 19\n",
            "Current loss value: 1254627500.0\n",
            "Image saved as /content/output/output_at_iteration_19.png\n",
            "Iteration 19 completed in 500s\n",
            "Start of iteration 20\n",
            "Current loss value: 1229367000.0\n",
            "Image saved as /content/output/output_at_iteration_20.png\n",
            "Iteration 20 completed in 485s\n",
            "Start of iteration 21\n",
            "Current loss value: 1207902700.0\n",
            "Image saved as /content/output/output_at_iteration_21.png\n",
            "Iteration 21 completed in 480s\n",
            "Start of iteration 22\n",
            "Current loss value: 1190545700.0\n",
            "Image saved as /content/output/output_at_iteration_22.png\n",
            "Iteration 22 completed in 481s\n",
            "Start of iteration 23\n",
            "Current loss value: 1178077800.0\n",
            "Image saved as /content/output/output_at_iteration_23.png\n",
            "Iteration 23 completed in 478s\n",
            "Start of iteration 24\n"
          ]
        }
      ]
    }
  ]
}